{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "121936d2",
   "metadata": {},
   "source": [
    "## optimizer 최적화를 수행하는 알고리즘\n",
    "```\n",
    "손실함수의 최솟값 찾는 과정\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1275b73f",
   "metadata": {},
   "source": [
    "### BGD : 배치 경사 하강법\n",
    "```\n",
    "전체 데이터셋에 대해 한번에 학습\n",
    "\n",
    "* 학습시간 매우 길어짐\n",
    "\n",
    "* 지역 최솟값을 빠져나오기 어려움\n",
    "```\n",
    "### SGD : 확률 경사 하강법\n",
    "```\n",
    "전체 학습 데이터 중 특정 크기만큼 임의로 선택해서 학습\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03c66af",
   "metadata": {},
   "source": [
    "### 미니 배치\n",
    "```\n",
    "epoch : 전체 데이터셋이 한번 최적화를 수행한 것\n",
    "iteration : 하나의 배치가 한번 최적화를 수행한 것\n",
    "\n",
    "전체 데이터셋 : 1000개, 배치 사이즈 : 200개\n",
    "\n",
    "각 200개의 서브 데이터셋 = 미니배치\n",
    "\n",
    "1 epoch => 5 iteration\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995d3ab8",
   "metadata": {},
   "source": [
    "### Local minima\n",
    "```\n",
    "지역 최소점, 주변 cost 함수값 중 최소값을 가지는 지점\n",
    "```\n",
    "### Global minima\n",
    "```\n",
    "전역 최소점, 전체 cost 함수값 중 최소값을 가지는 지점\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90162d6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.7513], grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.rand(1, requires_grad=True)\n",
    "y = torch.rand(1, requires_grad=True)\n",
    "\n",
    "loss = y-x\n",
    "\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06f77dd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-1.]), tensor([1.]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.backward()\n",
    "x.grad, y.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef244c83",
   "metadata": {},
   "source": [
    "## autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed76fe65",
   "metadata": {},
   "source": [
    "### W.grad : 가중치 미분값\n",
    "### b.grad : 상수 미분값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e55872af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4967, 0.4260, 0.7743],\n",
      "        [0.8774, 0.7788, 0.8450],\n",
      "        [0.7319, 0.6691, 0.0249],\n",
      "        [0.2951, 0.1788, 0.1588]], requires_grad=True) tensor([0.2347, 0.8928, 0.7382], requires_grad=True) tensor([2.6359, 2.9455, 2.5413], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 입력, 출력, 가중치. b값 모두 생성\n",
    "\n",
    "import torch\n",
    "\n",
    "x = torch.ones(4)  # input tensor\n",
    "y = torch.zeros(3)  # expected output\n",
    "W = torch.rand(4, 3, requires_grad=True)\n",
    "b = torch.rand(3, requires_grad=True)\n",
    "z = torch.matmul(x, W) + b\n",
    "print (W, b, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c7185af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7.3605, grad_fn=<MseLossBackward0>) tensor([[1.7572, 1.9636, 1.6942],\n",
      "        [1.7572, 1.9636, 1.6942],\n",
      "        [1.7572, 1.9636, 1.6942],\n",
      "        [1.7572, 1.9636, 1.6942]]) tensor([1.7572, 1.9636, 1.6942])\n"
     ]
    }
   ],
   "source": [
    "# loss.backward 하는 순간 역전파 시작, w1 미분 부터 다시 계산\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "loss = F.mse_loss(z, y)\n",
    "loss.backward()\n",
    "print(loss, W.grad, b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2075d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 tensor(7.3605, grad_fn=<MseLossBackward0>) tensor([2.6359, 2.9455, 2.5413], grad_fn=<AddBackward0>) tensor([0., 0., 0.])\n",
      "2 tensor(3.2713, grad_fn=<MseLossBackward0>) tensor([1.7572, 1.9636, 1.6942], grad_fn=<AddBackward0>) tensor([0., 0., 0.])\n",
      "3 tensor(1.4539, grad_fn=<MseLossBackward0>) tensor([1.1715, 1.3091, 1.1294], grad_fn=<AddBackward0>) tensor([0., 0., 0.])\n",
      "4 tensor(0.6462, grad_fn=<MseLossBackward0>) tensor([0.7810, 0.8727, 0.7530], grad_fn=<AddBackward0>) tensor([0., 0., 0.])\n",
      "5 tensor(0.2872, grad_fn=<MseLossBackward0>) tensor([0.5207, 0.5818, 0.5020], grad_fn=<AddBackward0>) tensor([0., 0., 0.])\n",
      "6 tensor(0.1276, grad_fn=<MseLossBackward0>) tensor([0.3471, 0.3879, 0.3347], grad_fn=<AddBackward0>) tensor([0., 0., 0.])\n",
      "7 tensor(0.0567, grad_fn=<MseLossBackward0>) tensor([0.2314, 0.2586, 0.2231], grad_fn=<AddBackward0>) tensor([0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.1\n",
    "learning_rate = 0.1\n",
    "iteration_num = 0\n",
    "\n",
    "# 최적화 완료한 in[37]\n",
    "# 다시 한번 역전파 최적화\n",
    "\n",
    "while loss > threshold:\n",
    "    iteration_num += 1\n",
    "    W = W - learning_rate * W.grad\n",
    "    b = b - learning_rate * b.grad\n",
    "    print(iteration_num, loss, z, y)\n",
    "    \n",
    "    W.detach_().requires_grad_(True)\n",
    "    b.detach_().requires_grad_(True)\n",
    "    \n",
    "    z = torch.matmul(x, W) + b\n",
    "    loss = F.mse_loss(z, y)\n",
    "    loss.backward()\n",
    "    \n",
    "print(iteration_num + 1, loss, z, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "985395f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "    \n",
    "model = LinearRegressionModel(4, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f68bac60",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(4)\n",
    "y = torch.zeros(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d678c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "nb_epochs = 1000 \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(nb_epochs + 1):\n",
    "\n",
    "    pred = model(x)\n",
    "    loss = F.mse_loss(pred, y)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f8565d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6.0256e-13, grad_fn=<MseLossBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[-0.0788,  0.2903,  0.1098, -0.4175],\n",
      "        [-0.2188, -0.3833,  0.3814, -0.1126],\n",
      "        [ 0.3285,  0.1401, -0.1803, -0.1561]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0962,  0.3334, -0.1322], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(loss)\n",
    "for param in model.parameters():\n",
    "    print (param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "240cf75c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _diabetes_dataset:\n",
      "\n",
      "Diabetes dataset\n",
      "----------------\n",
      "\n",
      "Ten baseline variables, age, sex, body mass index, average blood\n",
      "pressure, and six blood serum measurements were obtained for each of n =\n",
      "442 diabetes patients, as well as the response of interest, a\n",
      "quantitative measure of disease progression one year after baseline.\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "  :Number of Instances: 442\n",
      "\n",
      "  :Number of Attributes: First 10 columns are numeric predictive values\n",
      "\n",
      "  :Target: Column 11 is a quantitative measure of disease progression one year after baseline\n",
      "\n",
      "  :Attribute Information:\n",
      "      - age     age in years\n",
      "      - sex\n",
      "      - bmi     body mass index\n",
      "      - bp      average blood pressure\n",
      "      - s1      tc, total serum cholesterol\n",
      "      - s2      ldl, low-density lipoproteins\n",
      "      - s3      hdl, high-density lipoproteins\n",
      "      - s4      tch, total cholesterol / HDL\n",
      "      - s5      ltg, possibly log of serum triglycerides level\n",
      "      - s6      glu, blood sugar level\n",
      "\n",
      "Note: Each of these 10 feature variables have been mean centered and scaled by the standard deviation times the square root of `n_samples` (i.e. the sum of squares of each column totals 1).\n",
      "\n",
      "Source URL:\n",
      "https://www4.stat.ncsu.edu/~boos/var.select/diabetes.html\n",
      "\n",
      "For more information see:\n",
      "Bradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani (2004) \"Least Angle Regression,\" Annals of Statistics (with discussion), 407-499.\n",
      "(https://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_diabetes\n",
    "diabetes_data = load_diabetes()\n",
    "\n",
    "# 데이터 세트에 대한 전체 설명\n",
    "print (diabetes_data.DESCR) \n",
    "\n",
    "x = torch.from_numpy(np.array(diabetes_data.data[:, :-1], dtype=np.float32))\n",
    "y = torch.from_numpy(np.array(diabetes_data.data[:, [-1]], dtype=np.float32))\n",
    "#shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a3ef676f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "    \n",
    "model = LinearRegressionModel(x.size(1), y.size(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1dea711a",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "nb_epochs = 10000\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(nb_epochs + 1):\n",
    "\n",
    "    y_pred = model(x)\n",
    "    loss = F.mse_loss(y_pred, y)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "51483dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0022, grad_fn=<MseLossBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0370,  0.2380, -0.1211, -0.0592,  0.0807,  0.2076, -0.1292,  0.2640,\n",
      "         -0.0649]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-2.2402e-10], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(loss)\n",
    "for param in model.parameters():\n",
    "    print (param)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
